{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt-get install pandoc texlive-xetex texlive-fonts-recommended texlive-plain-generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!jupyter nbconvert --to pdf --LatexPreprocessor.title \"Assigment 2\" --LatexPreprocessor.author_names Joao\\ Gabriel --LatexPreprocessor.author_names Fahime --LatexPreprocessor.date \"May 21th 2025\" spark-task.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages\n",
    "\n",
    "Here we import the pyspark packages (installed with pip) which is necessary to access the spark funtions using python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Context\n",
    "\n",
    "* pyspark.SparkContext is an entry point to the PySpark. It creates the access to the SparkContext within the pySpark shell.\n",
    "\n",
    "* The SparkContext is the main entry point for Spark. A SparkContext represents the connection to a Spark cluster and lets you interact with them using the spark functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/22 09:51:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(\"local\", \"spark-task\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-1 (a)\n",
    "\n",
    "Start by creating a list of 0 to 40000, afterwards use parallelize() to make an RDD\n",
    "from the list (distribute your list into four partitions). Use RRD’s filter and a\n",
    "lambda function to convert your list into a list of odd numbers. Show the result by\n",
    "using the take() function and output the first seven numbers in the odds list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The parallelize function distributes a local Python collection to form an RDD.\n",
    "\n",
    "* An Resilient Distributed Dataset (RDD) is an essential structure within Spark.\n",
    "\n",
    "* It represents an immutable distributed collection of elements of your data partitioned across nodes of the spark cluster.\n",
    "\n",
    "* The filter function from the RDD creates a new RDD by selecting the elements that satisfy a given condition.\n",
    "\n",
    "* In this case the condition is given by the lamda funtion.\n",
    "\n",
    "* The Spark cluster is a collection of processes (workers) that execute functionalities from Spark to the RDD in pararallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list type:  <class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkList = list(range(0,40001))\n",
    "print(\"list type: \", type(sparkList[0]))\n",
    "rdd = sc.parallelize(sparkList)\n",
    "odd_rdd = rdd.filter(lambda x: x%2 == 1)\n",
    "odd_rdd.take(7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-1(b)\n",
    "\n",
    "Create an RDD with 10,000 random integers between 1 and 100. Partition the RDD into 5 partitions. Count how many numbers fall into each of the following buckets: 0-20, 21-40, 41-60, 61-80, 81-100. Show the results as key-value pairs: (”1-20”, count), (”21-40”, count), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this taks the RDD is created using a predefined number of buckets - 5.\n",
    "\n",
    "* The buckets partition the RDD in 5 sections in order to parallelize the RDD functionalities using them in parallel.\n",
    "\n",
    "* Here we use the map function to map each number within the category needed.\n",
    "\n",
    "* Then we use reduceByKey in order to reduce the numbers in groups of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=======================>                                   (2 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1-20, 2018)\n",
      "(21-40, 2007)\n",
      "(41-60, 1989)\n",
      "(61-80, 1954)\n",
      "(81-100, 2032)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_list = [random.randint(1,100) for _ in range(10000)]\n",
    "random_rdd = sc.parallelize(random_list, 5)\n",
    "def bucket(num):\n",
    "\n",
    "    if 1<= num <= 20:\n",
    "        return \"1-20\"\n",
    "    elif 21<= num <= 40:\n",
    "        return \"21-40\"\n",
    "    elif 41<= num <= 60:\n",
    "        return \"41-60\"\n",
    "    elif 61<= num <= 80:\n",
    "        return \"61-80\"\n",
    "    elif 81<= num <= 100:\n",
    "        return \"81-100\"\n",
    "    else:\n",
    "        return \"OutOfRange\"\n",
    "\n",
    "counts = (random_rdd.map(lambda x: (bucket(x),1)).reduceByKey(lambda a,b: a+b))\n",
    "\n",
    "res = counts.collect()\n",
    "\n",
    "for bucket_name, count in sorted(res):\n",
    "    print(f\"({bucket_name}, {count})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-1 (c):\n",
    "\n",
    "Explain what the parallelize() function does. Define RDD and describe the process\n",
    "used step-by-step in part (a). After that, compare this approach with filtering even\n",
    "numbers in a list using Python’s built-in filter() function (as done in the previous\n",
    "assignment). Highlight the differences between using RDD’s filter() and Python’s\n",
    "filter() in terms of execution and scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The parallelize function distributes a local Python collection to form an RDD.\n",
    "\n",
    "* An Resilient Distributed Dataset (RDD) is an essential structure within Spark.\n",
    "\n",
    "* It represents an immutable distributed collection of elements of your data partitioned across nodes of the spark cluster.\n",
    "\n",
    "* The filter function from the RDD creates a new RDD by selecting the elements that satisfy a given condition.\n",
    "\n",
    "* In this case the condition is given by the lamda funtion.\n",
    "\n",
    "* The Spark cluster is a collection of processes (workers) that execute functionalities from Spark to the RDD in pararallel.\n",
    "\n",
    "* The difference between the python filter and the Spark filter is that Spark parallelize this functionality.\n",
    "\n",
    "* To do this, Spark works with several partitions of the entire data and run the funtionalities across the partitions at the same time.\n",
    "\n",
    "* This makes it possible to work with big datasets without having to process the entire data everytime a functinonality is called."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-1(d)\n",
    "\n",
    "Count the number of appearances of each word in the text file ”Text-Data” using PySpark. Show the results of the 25 words with the most occurrences as a tuple (e.g., (”word”, 23))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we use a flat map the text into words.\n",
    "\n",
    "* The regular expression returns only words without punctuation and in lower case.\n",
    "\n",
    "* Then a map function is used to categorize data with the same value (repeated words).\n",
    "\n",
    "* Then a reduceByKey function count how many values fall in the same category.\n",
    "\n",
    "* Finally, the 25 most repeated words are taken by a takeOrdered function, that takes 25 groups and its cocunt value after ordering them in relation to their count value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('is', 144276)\n",
      "('the', 78015)\n",
      "('a', 77125)\n",
      "('of', 70999)\n",
      "('to', 43196)\n",
      "('or', 35443)\n",
      "('in', 31537)\n",
      "('and', 27278)\n",
      "('an', 14550)\n",
      "('that', 13839)\n",
      "('with', 13356)\n",
      "('by', 11357)\n",
      "('for', 11048)\n",
      "('on', 7501)\n",
      "('as', 6783)\n",
      "('from', 6782)\n",
      "('who', 6003)\n",
      "('having', 5808)\n",
      "('he', 4824)\n",
      "('used', 4803)\n",
      "('was', 4359)\n",
      "('his', 3990)\n",
      "('one', 3808)\n",
      "('s', 3683)\n",
      "('at', 3630)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text_rdd = sc.textFile(\"Text-data.txt\")\n",
    "words_rdd = text_rdd.flatMap(lambda line: re.findall(r'\\b\\w+\\b', line.lower())) # Split each line into words --> no punctuation, lower case\n",
    "\n",
    "# I need word-pairs. Use map function\n",
    "words_pairs = words_rdd.map(lambda x: (x,1))\n",
    "\n",
    "# sum to same words. Use ReduceByKey()\n",
    "\n",
    "same_words = words_pairs.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "# 25 most common words\n",
    "top_25 = same_words.takeOrdered(25, key=lambda x: -x[1]) \n",
    "# If we didn't delete the punctuation mark, this line would give an error.\n",
    "\n",
    "for word, count in top_25:\n",
    "    print((word, count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-1 (e)\n",
    "\n",
    "List all those words, which can be classified as ”auxiliary verbs” or ”function words” to exclude them from your results (e.g. ”by”, ”to”, ”and” are ”not-meaningful-words”). Order each ”meaningful” word by occurrence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First we create a list of words that we consider as ”auxiliary verbs” or ”function words”.\n",
    "\n",
    "* Then we filter the RDD by excluding these words.\n",
    "\n",
    "* After filtering we count how many meaningful words there are by using takeOrdered again.\n",
    "\n",
    "* Finally, we print the 25 most counted meaninful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('having', 5808)\n",
      "('he', 4824)\n",
      "('used', 4803)\n",
      "('his', 3990)\n",
      "('one', 3808)\n",
      "('s', 3683)\n",
      "('small', 3145)\n",
      "('genus', 3121)\n",
      "('which', 2835)\n",
      "('united', 2789)\n",
      "('states', 2749)\n",
      "('relating', 2666)\n",
      "('it', 2613)\n",
      "('something', 2335)\n",
      "('large', 2223)\n",
      "('person', 2214)\n",
      "('she', 2189)\n",
      "('manner', 1968)\n",
      "('her', 1966)\n",
      "('two', 1866)\n",
      "('made', 1843)\n",
      "('act', 1822)\n",
      "('someone', 1812)\n",
      "('its', 1731)\n",
      "('north', 1666)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# set stopwords\n",
    "\n",
    "stop_words = {\n",
    "    \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"if\", \"while\", \"because\", \"as\", \"until\", \"of\",\n",
    "    \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\",\n",
    "    \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\",\n",
    "    \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\",\n",
    "    \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
    "    \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\",\n",
    "    \"can\", \"will\", \"just\", \"don\", \"should\", \"now\", \"is\", \"am\", \"are\", \"was\", \"were\",\n",
    "    \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"would\", \"could\",\n",
    "    \"may\", \"might\", \"must\", \"shall\", \"also\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"you\"\n",
    "}\n",
    "\n",
    "filtered_words = words_rdd.filter(lambda x: x not in stop_words)\n",
    "meaningful_words_count = filtered_words.map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b)\n",
    "top_25_meaningful_words = meaningful_words_count.takeOrdered(25, key=lambda x: -x[1])\n",
    "for meaningful_word, count in top_25_meaningful_words:\n",
    "    print((meaningful_word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop SparkContext\n",
    "\n",
    "Finally we realease the sparkContext since we are not going to use it anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
